{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangcuongnguyen2001/Honours_Repository/blob/main/SciBERT_train_single_label.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT_G4jp4zkC8"
      },
      "source": [
        "This notebook documents the procedure for training the single-label models during the 2023 TRAM effort.\n",
        "\n",
        "The `bootstrap-training-data` file contains the annotations that existed prior, as well as the annotations that were produced during the 2023 effort."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "#Upload the JSON file for fine-tuning (from cti-to-mitre-with-nlp)\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/file/d/1BCkEdKgmH49kjihmrxlXVUQvB0GsiQJW/view?usp=drive_link'\n",
        "output_path = 'TRAM_fine_tuned_SciBERT.json'\n",
        "gdown.download(url, output_path, quiet=False,fuzzy=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "vzc4v4S8zwup",
        "outputId": "467c3a91-d708-4427-81b4-e26d6ebfbdae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BCkEdKgmH49kjihmrxlXVUQvB0GsiQJW\n",
            "To: /content/TRAM_fine_tuned_SciBERT.json\n",
            "100%|██████████| 1.38M/1.38M [00:00<00:00, 74.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TRAM_fine_tuned_SciBERT.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_zAwJ29zkC-",
        "outputId": "4ec87fd7-454a-4c27-dbf9-e378ac42f298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text      label\n",
            "0     Anchor has used cmd.exe to run its self deleti...  T1059.003\n",
            "1     Zeus Panda can launch an interface where it ca...  T1059.003\n",
            "2     Chimera has used the Windows Command Shell and...  T1059.003\n",
            "3     Cuba has used cmd.exe /c and batch files for e...  T1059.003\n",
            "4     MechaFlounder has the ability to run commands ...  T1059.003\n",
            "...                                                 ...        ...\n",
            "9806                           Kazuar can delete files.  T1070.004\n",
            "9807  Cobalt Strike can exploit vulnerabilities such...      T1068\n",
            "9808  QakBot can send stolen information to C2 nodes...      T1041\n",
            "9809  Turla RPC backdoors have also searched for fil...      T1083\n",
            "9810  Ramsay has created Registry Run keys to establ...  T1547.001\n",
            "\n",
            "[9811 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "with open('TRAM_fine_tuned_SciBERT.json') as f:\n",
        "    data = json.loads(f.read())\n",
        "\n",
        "data = pd.DataFrame(\n",
        "    [\n",
        "        {'text': row['text'], 'label': row['label']}\n",
        "        for row in data\n",
        "\n",
        "    ]\n",
        ")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqtjxR100aVo",
        "outputId": "511ddf6c-5487-47c7-bab2-fb14912f16e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uot_hIlzkC_"
      },
      "source": [
        "We then load the model and move it to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7A_CJQlzkDA",
        "outputId": "4cb066bc-76f2-445b-829a-6121cfa0f7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "mode: 'bert or gpt' = 'bert'\n",
        "cuda = torch.device('cuda')\n",
        "\n",
        "if mode == 'bert':\n",
        "    model = transformers.BertForSequenceClassification.from_pretrained(\n",
        "        \"allenai/scibert_scivocab_uncased\",\n",
        "        num_labels=data['label'].nunique(),\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "    )\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", max_length=512)\n",
        "elif mode == 'gpt':\n",
        "    model = transformers.GPT2ForSequenceClassification.from_pretrained(\n",
        "        \"gpt2\",\n",
        "        num_labels=data['label'].nunique(),\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "    )\n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\", max_length=512)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "else:\n",
        "    raise ValueError(f\"mode must be one of bert or gpt, but is {mode = !r}\")\n",
        "\n",
        "model.train().to(cuda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfcV3kJlzkDA"
      },
      "source": [
        "We will represent the labels using one hot encoding.\n",
        "\n",
        "The `apply_attention_mask` function returns an attention mask (which is a tensor) where the element for every non-padding token is `1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "-HX8Z_5dzkDA"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder as OHE\n",
        "\n",
        "encoder = OHE(sparse_output=False)\n",
        "encoder.fit(data[['label']])\n",
        "\n",
        "def tokenize(samples: 'list[str]'):\n",
        "    return tokenizer(samples, return_tensors='pt', padding='max_length', truncation=True, max_length=512).input_ids\n",
        "\n",
        "def load_data(x, y, batch_size=10):\n",
        "    x_len, y_len = x.shape[0], y.shape[0]\n",
        "    assert x_len == y_len\n",
        "    for i in range(0, x_len, batch_size):\n",
        "        slc = slice(i, i + batch_size)\n",
        "        yield x[slc].to(cuda), y[slc].to(cuda)\n",
        "\n",
        "def apply_attention_mask(x):\n",
        "    return x.ne(tokenizer.pad_token_id).to(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3Kqup25zkDA",
        "outputId": "e2c8ab73-369b-4882-aa49-3a1b8c828641"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  102, 21159, 22229,  ...,     0,     0,     0],\n",
              "        [  102, 16660,  5688,  ...,     0,     0,     0],\n",
              "        [  102, 20600,   434,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  102,  1365, 29887,  ...,     0,     0,     0],\n",
              "        [  102, 25179,   282,  ...,     0,     0,     0],\n",
              "        [  102,  1109, 11277,  ...,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=.2, stratify=data['label'])\n",
        "\n",
        "x_train = tokenize(train['text'].tolist())\n",
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVQFdyBmJTwm",
        "outputId": "7a373e19-ea08-478f-9a9f-c943f91d4b28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvGs0f0zzkDB",
        "outputId": "823b3eae-8897-4968-bf50-cd4890183b13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "y_train = torch.Tensor(encoder.transform(train[['label']]))\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tokenize(test['text'].tolist())\n",
        "y_test = torch.Tensor(encoder.transform(test[['label']]))"
      ],
      "metadata": {
        "id": "6io6mU9pHCvl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9U65Xn1zkDB"
      },
      "source": [
        "The hyperparameters shown here are those that we used, including the number of epochs and batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMy7Vm-hzkDB",
        "outputId": "8e8138b4-85a3-419b-ab46-c589072466c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "785it [11:33,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 loss: 0.12712413937233055\n",
            "Validation for epoch 1 loss: 0.09163273617549596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "785it [11:33,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 loss: 0.07100867721590266\n",
            "Validation for epoch 2 loss: 0.05098084413309388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "785it [11:33,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 loss: 0.039829693760746604\n",
            "Validation for epoch 3 loss: 0.03323306162193947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "785it [11:33,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 loss: 0.02330432324106716\n",
            "Validation for epoch 4 loss: 0.025906124336616643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "785it [11:33,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 loss: 0.014950301550376188\n",
            "Validation for epoch 5 loss: 0.02196513876545853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "785it [11:33,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 loss: 0.009599128989562108\n",
            "Validation for epoch 6 loss: 0.023518645121871972\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "for epoch in range(6):\n",
        "    epoch_losses = []\n",
        "    validation_loss = []\n",
        "    model.train()\n",
        "    for x, y in tqdm(load_data(x_train, y_train, batch_size=10)):\n",
        "        model.zero_grad()\n",
        "        out = model(x, attention_mask=apply_attention_mask(x), labels=y)\n",
        "        epoch_losses.append(out.loss.item())\n",
        "        out.loss.backward()\n",
        "        optim.step()\n",
        "    print(f\"epoch {epoch + 1} loss: {mean(epoch_losses)}\")\n",
        "    # Free up GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "    model.eval()\n",
        "    for x, y in load_data(x_test, y_test, batch_size=10):\n",
        "\n",
        "    # validation data\n",
        "        validation_output = model(x, attention_mask=apply_attention_mask(x), labels=y)\n",
        "        validation_loss.append(validation_output.loss.item())\n",
        "\n",
        "    print(f\"Validation for epoch {epoch + 1} loss: {mean(validation_loss)}\")\n",
        "    # Free up GPU memory\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oPafNEkzkDB",
        "outputId": "0bc84a5d-67ad-451f-b07f-6f5a5d87d342"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['T1566.001', 'T1053.005', 'T1110', ..., 'T1573.001', 'T1005',\n",
              "       'T1083'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "model.eval()\n",
        "\n",
        "preds = []\n",
        "batch_size = 20\n",
        "\n",
        "x_test = tokenize(test['text'].tolist())\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, x_test.shape[0], batch_size):\n",
        "        x = x_test[i : i + batch_size].to(cuda)\n",
        "        out = model(x, attention_mask=apply_attention_mask(x))\n",
        "        preds.extend(out.logits.to('cpu'))\n",
        "\n",
        "predicted_labels = (\n",
        "    encoder.inverse_transform(\n",
        "        F.one_hot(\n",
        "            torch.vstack(preds).softmax(-1).argmax(-1),\n",
        "            num_classes=50\n",
        "        )\n",
        "        .numpy()\n",
        "    )\n",
        "    .reshape(-1)\n",
        ")\n",
        "\n",
        "predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [],
        "id": "8JED1nItzkDB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6eaad47f-7f91-49b8-9c2c-74a2bc09fd53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  P         R        F1     #\n",
              "T1003.001  0.888889  0.666667  0.761905  36.0\n",
              "T1005      0.833333  0.681818  0.750000  22.0\n",
              "T1012      0.888889  0.842105  0.864865  38.0\n",
              "T1016      0.975000  0.780000  0.866667  50.0\n",
              "T1021.001  0.827586  0.727273  0.774194  33.0\n",
              "T1027      0.792453  0.700000  0.743363  60.0\n",
              "T1033      0.941176  0.864865  0.901408  37.0\n",
              "T1036.005  0.718750  0.741935  0.730159  62.0\n",
              "T1041      0.829268  0.871795  0.850000  39.0\n",
              "T1047      0.740741  0.909091  0.816327  22.0\n",
              "T1053.005  0.823529  0.800000  0.811594  35.0\n",
              "T1055      0.934783  0.781818  0.851485  55.0\n",
              "T1056.001  0.825000  0.891892  0.857143  37.0\n",
              "T1057      1.000000  0.847826  0.917647  46.0\n",
              "T1059.003  0.775510  0.962025  0.858757  79.0\n",
              "T1068      1.000000  0.416667  0.588235  12.0\n",
              "T1070.004  0.911765  0.815789  0.861111  76.0\n",
              "T1071.001  0.840426  0.908046  0.872928  87.0\n",
              "T1072      0.937500  0.833333  0.882353  18.0\n",
              "T1074.001  0.777778  0.700000  0.736842  20.0\n",
              "T1078      0.750000  0.833333  0.789474  18.0\n",
              "T1082      0.855072  0.867647  0.861314  68.0\n",
              "T1083      0.895833  0.728814  0.803738  59.0\n",
              "T1090      0.777778  0.807692  0.792453  26.0\n",
              "T1095      0.823529  0.933333  0.875000  30.0\n",
              "T1105      0.793478  0.890244  0.839080  82.0\n",
              "T1106      0.647059  0.846154  0.733333  26.0\n",
              "T1110      0.911765  0.911765  0.911765  34.0\n",
              "T1112      0.700000  0.608696  0.651163  23.0\n",
              "T1113      0.852941  1.000000  0.920635  29.0\n",
              "T1140      0.833333  0.882353  0.857143  51.0\n",
              "T1190      0.916667  1.000000  0.956522  22.0\n",
              "T1204.002  0.787234  0.860465  0.822222  43.0\n",
              "T1210      0.962963  1.000000  0.981132  26.0\n",
              "T1218.011  0.925926  0.943396  0.934579  53.0\n",
              "T1219      0.875000  1.000000  0.933333  21.0\n",
              "T1484.001  1.000000  1.000000  1.000000  21.0\n",
              "T1518.001  0.880000  0.709677  0.785714  31.0\n",
              "T1543.003  0.866667  0.666667  0.753623  39.0\n",
              "T1547.001  0.733333  0.927711  0.819149  83.0\n",
              "T1548.002  0.633333  0.974359  0.767677  39.0\n",
              "T1552.001  0.882353  0.625000  0.731707  24.0\n",
              "T1557.001  0.800000  0.869565  0.833333  23.0\n",
              "T1562.001  0.666667  0.777778  0.717949  36.0\n",
              "T1564.001  0.636364  0.800000  0.708861  35.0\n",
              "T1566.001  1.000000  0.750000  0.857143  48.0\n",
              "T1569.002  0.500000  0.538462  0.518519  13.0\n",
              "T1570      1.000000  0.625000  0.769231  16.0\n",
              "T1573.001  0.911765  0.756098  0.826667  41.0\n",
              "T1574.002  0.903226  0.717949  0.800000  39.0\n",
              "(micro)    0.827815  0.827815  0.827815   NaN\n",
              "(macro)    0.839693  0.811902  0.816989   NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37e8f3f4-550d-4701-a6f5-9bf7c34ce7a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P</th>\n",
              "      <th>R</th>\n",
              "      <th>F1</th>\n",
              "      <th>#</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T1003.001</th>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1005</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1012</th>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>38.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1016</th>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1021.001</th>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.774194</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1027</th>\n",
              "      <td>0.792453</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.743363</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1033</th>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.901408</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1036.005</th>\n",
              "      <td>0.718750</td>\n",
              "      <td>0.741935</td>\n",
              "      <td>0.730159</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1041</th>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.871795</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1047</th>\n",
              "      <td>0.740741</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.816327</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1053.005</th>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.811594</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1055</th>\n",
              "      <td>0.934783</td>\n",
              "      <td>0.781818</td>\n",
              "      <td>0.851485</td>\n",
              "      <td>55.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1056.001</th>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1057</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.847826</td>\n",
              "      <td>0.917647</td>\n",
              "      <td>46.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1059.003</th>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.962025</td>\n",
              "      <td>0.858757</td>\n",
              "      <td>79.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1068</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1070.004</th>\n",
              "      <td>0.911765</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>76.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1071.001</th>\n",
              "      <td>0.840426</td>\n",
              "      <td>0.908046</td>\n",
              "      <td>0.872928</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1072</th>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1074.001</th>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1078</th>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1082</th>\n",
              "      <td>0.855072</td>\n",
              "      <td>0.867647</td>\n",
              "      <td>0.861314</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1083</th>\n",
              "      <td>0.895833</td>\n",
              "      <td>0.728814</td>\n",
              "      <td>0.803738</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1090</th>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.792453</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1095</th>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1105</th>\n",
              "      <td>0.793478</td>\n",
              "      <td>0.890244</td>\n",
              "      <td>0.839080</td>\n",
              "      <td>82.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1106</th>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1110</th>\n",
              "      <td>0.911765</td>\n",
              "      <td>0.911765</td>\n",
              "      <td>0.911765</td>\n",
              "      <td>34.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1112</th>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.608696</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1113</th>\n",
              "      <td>0.852941</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.920635</td>\n",
              "      <td>29.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1140</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1190</th>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1204.002</th>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.860465</td>\n",
              "      <td>0.822222</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1210</th>\n",
              "      <td>0.962963</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.981132</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1218.011</th>\n",
              "      <td>0.925926</td>\n",
              "      <td>0.943396</td>\n",
              "      <td>0.934579</td>\n",
              "      <td>53.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1219</th>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1484.001</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1518.001</th>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.709677</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1543.003</th>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.753623</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1547.001</th>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.927711</td>\n",
              "      <td>0.819149</td>\n",
              "      <td>83.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1548.002</th>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.974359</td>\n",
              "      <td>0.767677</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1552.001</th>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.731707</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1557.001</th>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1562.001</th>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.717949</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1564.001</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.708861</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1566.001</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>48.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1569.002</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.518519</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1570</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1573.001</th>\n",
              "      <td>0.911765</td>\n",
              "      <td>0.756098</td>\n",
              "      <td>0.826667</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T1574.002</th>\n",
              "      <td>0.903226</td>\n",
              "      <td>0.717949</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(micro)</th>\n",
              "      <td>0.827815</td>\n",
              "      <td>0.827815</td>\n",
              "      <td>0.827815</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(macro)</th>\n",
              "      <td>0.839693</td>\n",
              "      <td>0.811902</td>\n",
              "      <td>0.816989</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37e8f3f4-550d-4701-a6f5-9bf7c34ce7a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-37e8f3f4-550d-4701-a6f5-9bf7c34ce7a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-37e8f3f4-550d-4701-a6f5-9bf7c34ce7a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d79a850b-906b-4e9c-a1e6-826ce4abb98c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d79a850b-906b-4e9c-a1e6-826ce4abb98c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d79a850b-906b-4e9c-a1e6-826ce4abb98c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as calculate_score\n",
        "\n",
        "predicted = list(predicted_labels)\n",
        "actual = test['label'].tolist()\n",
        "\n",
        "labels = sorted(data['label'].unique())\n",
        "\n",
        "scores = calculate_score(actual, predicted, labels=labels)\n",
        "\n",
        "scores_df = pd.DataFrame(scores).T\n",
        "scores_df.columns = ['P', 'R', 'F1', '#']\n",
        "scores_df.index = labels\n",
        "scores_df.loc['(micro)'] = calculate_score(actual, predicted, average='micro', labels=labels)\n",
        "scores_df.loc['(macro)'] = calculate_score(actual, predicted, average='macro', labels=labels)\n",
        "\n",
        "scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xiWrUyaiVKIP"
      },
      "outputs": [],
      "source": [
        "model = model.save_pretrained(\"scibert_model\")\n",
        "tokenizer = tokenizer.save_pretrained(\"scibert_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r scibert_model.zip scibert_model/\n",
        "!zip -r scibert_tokenizer.zip scibert_tokenizer/\n",
        "from google.colab import files\n",
        "files.download('scibert_model.zip')\n",
        "files.download('scibert_tokenizer.zip')"
      ],
      "metadata": {
        "id": "OKllDL_mGTvV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}